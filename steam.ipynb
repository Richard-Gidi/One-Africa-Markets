{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993da32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# OneAfrica Market Pulse ‚Äî Automated Market Intelligence (Streamlit Demo)\n",
    "# Author: Richard Gidi\n",
    "# Run: streamlit run streamlit_app.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import html\n",
    "import json\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import math\n",
    "import time  # NEW: for rate-limit sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "\n",
    "# NEW: For Grok integration (using openai-compatible client)\n",
    "from openai import OpenAI\n",
    "\n",
    "# ==== resilient .env loader ====\n",
    "try:\n",
    "    from dotenv import load_dotenv  # pip install python-dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==== OpenAI (lazy + resilient import) ====\n",
    "OPENAI_OK = True\n",
    "try:\n",
    "    from openai import OpenAI  # pip install openai==1.*\n",
    "except Exception:\n",
    "    OPENAI_OK = False\n",
    "\n",
    "# ========================= Streamlit safety: hide tracebacks =========================\n",
    "st.set_option('client.showErrorDetails', False)\n",
    "\n",
    "# ========================= Logging (server console only) =========================\n",
    "logger = logging.getLogger(\"oneafrica.pulse\")\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# ========================= Optional sklearn (graceful fallback) =========================\n",
    "HAS_SK = True\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "except Exception:\n",
    "    HAS_SK = False\n",
    "    logger.info(\"sklearn not available; falling back to keyword hit scoring.\")\n",
    "\n",
    "# ========================= Optional NLTK VADER for sentiment =========================\n",
    "HAS_VADER = True\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    try:\n",
    "        _ = nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"vader_lexicon\")\n",
    "except Exception as e:\n",
    "    HAS_VADER = False\n",
    "    logger.info(f\"VADER not available: {e}\")\n",
    "\n",
    "# ========================= NEW: Word export deps =========================\n",
    "from io import BytesIO\n",
    "from docx import Document\n",
    "from docx.shared import Pt, Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "# ========================= App Strings / Theme =========================\n",
    "APP_NAME = \"One Africa Market Pulse\"\n",
    "TAGLINE = \"Automated intelligence for cashew, shea, cocoa & allied markets.\"\n",
    "QUOTE = \"‚ÄúAsk your data why, until it has nothing else to say.‚Äù ‚Äî Richard Gidi\"\n",
    "\n",
    "# Fallback image for articles with no thumbnail\n",
    "FALLBACK_IMG = \"https://images.unsplash.com/photo-1519681393784-d120267933ba?q=80&w=1200&auto=format&fit=crop\"\n",
    "\n",
    "DEFAULT_KEYWORDS = [\n",
    "    \"cashew\", \"shea\", \"shea nut\", \"cocoa\", \"palm kernel\", \"agri\", \"export\", \"harvest\",\n",
    "    \"shipment\", \"freight\", \"logistics\", \"port\", \"tariff\", \"ban\", \"fx\", \"currency\",\n",
    "    \"cedi\", \"naira\", \"inflation\", \"subsidy\", \"cooperative\", \"value-addition\", \"processing\",\n",
    "    \"ghana\", \"nigeria\", \"cote d‚Äôivoire\", \"ivory coast\", \"benin\", \"togo\", \"burkina\",\n",
    "    \"west africa\", \"sahel\", \"trade policy\", \"commodity\", \"price\", \"market\"\n",
    "]\n",
    "\n",
    "# Curated working RSS/Atom sources\n",
    "DEFAULT_SOURCES = {\n",
    "    \"AllAfrica ¬ª Agriculture\": \"https://allafrica.com/tools/headlines/rdf/agriculture/headlines.rdf\",\n",
    "    \"AllAfrica ¬ª Business\": \"https://allafrica.com/tools/headlines/rdf/business/headlines.rdf\",\n",
    "    \"The Standard ¬ª Business\": \"https://www.standardmedia.co.ke/rss/business.php\",\n",
    "    \"The Standard ¬ª Agriculture\": \"https://www.standardmedia.co.ke/rss/agriculture.php\",\n",
    "    \"CitiNewsroom\": \"https://citinewsroom.com/feed/\",\n",
    "    \"FAO News (All topics)\": \"https://www.fao.org/news/rss/en\",\n",
    "    \"FAO GIEWS\": \"https://www.fao.org/giews/rss/en/\",\n",
    "    \"FreshPlaza Africa\": \"https://www.freshplaza.com/africa/rss.xml\",\n",
    "    \"African Arguments\": \"https://africanarguments.org/feed/\",\n",
    "    \"How We Made It In Africa\": \"https://www.howwemadeitinafrica.com/feed/\",\n",
    "    \"Bizcommunity (Africa ‚Ä¢ Agri+Logistics)\": \"https://www.bizcommunity.com/GenerateRss.aspx?i=63,76&c=81\",\n",
    "}\n",
    "\n",
    "IMPACT_RULES = {\n",
    "    \"Supply Risk\": [\n",
    "        r\"\\bexport (?:ban|restriction|control)\\b\",\n",
    "        r\"\\b(?:import|trade) (?:ban|restriction|control)\\b\",\n",
    "        r\"\\bembargo\\b\",\n",
    "        r\"\\b(?:drought|flood|rainfall|weather)\\b\",\n",
    "        r\"\\b(?:pest|disease|infestation)\\b\",\n",
    "        r\"\\b(?:shortage|scarcity)\\b\",\n",
    "        r\"\\b(?:strike|protest|unrest)\\b\",\n",
    "        r\"\\bport (?:closure|congestion|delay)\\b\",\n",
    "        r\"\\bharvest (?:delay|loss|damage)\\b\",\n",
    "        r\"\\bproduction (?:issue|problem|concern)\\b\",\n",
    "    ],\n",
    "    \"Price Upside\": [\n",
    "        r\"\\bstrong (?:demand|buying|interest)\\b\",\n",
    "        r\"\\b(?:surge|spike|jump|rise|increase)\\b\",\n",
    "        r\"\\b(?:grant|stimulus|support)\\b\",\n",
    "        r\"\\b(?:incentive|subsidy|funding)\\b\",\n",
    "        r\"\\bvalue[- ](?:addition|chain|processing)\\b\",\n",
    "        r\"\\bhigh(?:er)? (?:price|demand|consumption)\\b\",\n",
    "        r\"\\bmarket (?:rally|strength|upturn)\\b\",\n",
    "        r\"\\bsupply (?:squeeze|shortage|tightness)\\b\",\n",
    "        r\"\\bquality premium\\b\",\n",
    "    ],\n",
    "    \"Price Downside\": [\n",
    "        r\"\\b(?:oversupply|surplus|glut)\\b\",\n",
    "        r\"\\b(?:decline|fall|drop|decrease|slump)\\b\",\n",
    "        r\"\\b(?:weak|soft|bearish) (?:price|market|demand)\\b\",\n",
    "        r\"\\bcut (?:price|rate|cost)\\b\",\n",
    "        r\"\\blow(?:er)? (?:price|demand|consumption)\\b\",\n",
    "        r\"\\bmarket (?:weakness|downturn)\\b\",\n",
    "        r\"\\bcompetitive pressure\\b\",\n",
    "    ],\n",
    "    \"FX & Policy\": [\n",
    "        r\"\\b(?:depreciation|devaluation)\\b\",\n",
    "        r\"\\bweak(?:ening)? (?:currency|exchange)\\b\",\n",
    "        r\"\\b(?:fx|forex|dollar|euro|yuan)\\b\",\n",
    "        r\"\\b(?:monetary|fiscal|trade) policy\\b\",\n",
    "        r\"\\b(?:interest|exchange) rate\\b\",\n",
    "        r\"\\b(?:tariff|duty|levy|tax)\\b\",\n",
    "        r\"\\bregulatory (?:change|update|requirement)\\b\",\n",
    "        r\"\\bpolicy (?:change|update|reform)\\b\",\n",
    "    ],\n",
    "    \"Logistics & Trade\": [\n",
    "        r\"\\b(?:freight|shipping|transport)\\b\",\n",
    "        r\"\\b(?:port|container|vessel|cargo)\\b\",\n",
    "        r\"\\b(?:congestion|delay|bottleneck)\\b\",\n",
    "        r\"\\b(?:reroute|divert|alternative route)\\b\",\n",
    "        r\"\\b(?:cost|rate) (?:increase|surge|rise)\\b\",\n",
    "        r\"\\btrade (?:flow|route|pattern)\\b\",\n",
    "        r\"\\b(?:export|import) (?:volume|data|figure)\\b\",\n",
    "    ],\n",
    "    \"Market Structure\": [\n",
    "        r\"\\b(?:merger|acquisition|takeover)\\b\",\n",
    "        r\"\\b(?:investment|expansion|capacity)\\b\",\n",
    "        r\"\\b(?:processing|factory|facility)\\b\",\n",
    "        r\"\\b(?:certification|standard|quality)\\b\",\n",
    "        r\"\\b(?:cooperative|association|group)\\b\",\n",
    "        r\"\\b(?:contract|agreement|deal)\\b\",\n",
    "        r\"\\b(?:partnership|collaboration)\\b\",\n",
    "        r\"\\bmarket (?:structure|reform|development)\\b\",\n",
    "    ],\n",
    "    \"Tech & Innovation\": [\n",
    "        r\"\\b(?:technology|innovation|digital)\\b\",\n",
    "        r\"\\b(?:blockchain|traceability|tracking)\\b\",\n",
    "        r\"\\b(?:sustainability|sustainable)\\b\",\n",
    "        r\"\\b(?:efficiency|optimization)\\b\",\n",
    "        r\"\\b(?:automation|mechanization)\\b\",\n",
    "        r\"\\b(?:research|development|r&d)\\b\",\n",
    "        r\"\\b(?:startup|fintech|agtech)\\b\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ========================= Streamlit UI CSS =========================\n",
    "CARD_CSS = \"\"\"\n",
    "<style>\n",
    ".hero {\n",
    "  position: relative;\n",
    "  border-radius: 16px;\n",
    "  padding: 28px 28px;\n",
    "  background: linear-gradient(135deg, #0ea5e9, #7c3aed 60%);\n",
    "  color: white;\n",
    "  box-shadow: 0 14px 40px rgba(0,0,0,0.18);\n",
    "}\n",
    ".hero h1 { margin: 0 0 6px 0; font-size: 28px; font-weight: 800; }\n",
    ".hero p { margin: 0; opacity: .95; }\n",
    "\n",
    ".pill {\n",
    "  display: inline-flex; align-items: center; gap: 8px;\n",
    "  padding: 6px 12px; border-radius: 999px;\n",
    "  background: rgba(255,255,255,0.15); color:#fff; font-weight:600; font-size: 13px;\n",
    "}\n",
    "\n",
    ".card {\n",
    "  background: #ffffff;\n",
    "  border: 1px solid rgba(0,0,0,.06);\n",
    "  border-radius: 14px; overflow: hidden;\n",
    "  transition: transform .15s ease, box-shadow .15s ease;\n",
    "}\n",
    ".card:hover { transform: translateY(-3px); box-shadow: 0 10px 24px rgba(0,0,0,0.08); }\n",
    ".thumb { width: 100%; height: 180px; object-fit: cover; background:#f6f7f9; }\n",
    ".card-body { padding: 14px; }\n",
    ".card .title { color: #111827 !important; font-weight: 800; font-size: 18px; margin: 6px 0 8px 0; line-height: 1.25; }\n",
    ".card .meta { color: #6b7280 !important; font-size: 12px; display:flex; gap:10px; flex-wrap:wrap; margin-bottom:8px; }\n",
    ".card .summary { color:#374151 !important; font-size: 13px; line-height:1.55; margin-top: 6px; }\n",
    ".badges { display:flex; flex-wrap:wrap; gap:6px; margin:8px 0; }\n",
    ".badge { font-size: 11px; font-weight:700; padding:4px 8px; border-radius:999px; background:#eef2ff; color:#3730a3; border:1px solid #c7d2fe; }\n",
    ".link { text-decoration: none; font-weight:700; color:#2563eb !important; }\n",
    "</style>\n",
    "\"\"\"\n",
    "\n",
    "# ========================= Secrets helpers (no warnings) =========================\n",
    "def _get_secret_safely(name: str) -> str:\n",
    "    val = os.environ.get(name, \"\")\n",
    "    if val:\n",
    "        return str(val).strip().strip('\"').strip(\"'\")\n",
    "    try:\n",
    "        if hasattr(st, \"secrets\"):\n",
    "            try:\n",
    "                if len(st.secrets) > 0 and name in st.secrets:\n",
    "                    return str(st.secrets.get(name, \"\")).strip().strip('\"').strip(\"'\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "def get_newsdata_api_key() -> str:\n",
    "    return _get_secret_safely(\"NEWSDATA_API_KEY\")\n",
    "\n",
    "def get_openai_api_key() -> str:\n",
    "    return _get_secret_safely(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_twitter_bearer() -> str:\n",
    "    # X/Twitter v2 bearer token if available\n",
    "    return _get_secret_safely(\"TWITTER_BEARER_TOKEN\") or _get_secret_safely(\"X_BEARER_TOKEN\")\n",
    "\n",
    "def get_xai_api_key() -> str:\n",
    "    return _get_secret_safely(\"XAI_API_KEY\")\n",
    "\n",
    "# ========================= HTTP utils + safe wrappers =========================\n",
    "def get_session() -> requests.Session:\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "def _normalize(text: str) -> str:\n",
    "    text = html.unescape(text or \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# ---- Session helpers ----\n",
    "def ss_get(key, default):\n",
    "    if key not in st.session_state:\n",
    "        st.session_state[key] = default\n",
    "    return st.session_state[key]\n",
    "\n",
    "def ss_set(key, value):\n",
    "    st.session_state[key] = value\n",
    "\n",
    "# Global soft error bag\n",
    "SOFT_ERRORS: List[str] = []\n",
    "def soft_fail(msg: str, detail: Optional[str] = None):\n",
    "    if msg:\n",
    "        SOFT_ERRORS.append(msg)\n",
    "    if detail:\n",
    "        logger.warning(detail)\n",
    "\n",
    "# ========================= Content helpers =========================\n",
    "@st.cache_data(ttl=60*30, show_spinner=False)\n",
    "def fetch_page(url: str, timeout: int = 12) -> str:\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (compatible; OneAfricaPulse/1.0)\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        }\n",
    "        r = get_session().get(url, headers=headers, timeout=timeout)\n",
    "        if r.status_code != 200:\n",
    "            soft_fail(\"Skipped a page that didn‚Äôt load cleanly.\", f\"fetch_page {url} -> {r.status_code}\")\n",
    "            return \"\"\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Skipped one page due to connectivity.\", f\"fetch_page EXC {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_og_image(soup: BeautifulSoup, base_url: str) -> Optional[str]:\n",
    "    try:\n",
    "        candidates = [\n",
    "            (\"meta\", {\"property\": \"og:image\"}),\n",
    "            (\"meta\", {\"name\": \"twitter:image\"}),\n",
    "            (\"meta\", {\"property\": \"twitter:image\"}),\n",
    "            (\"link\", {\"rel\": \"image_src\"}),\n",
    "        ]\n",
    "        for tag, attrs in candidates:\n",
    "            el = soup.find(tag, attrs=attrs)\n",
    "            if el:\n",
    "                src = el.get(\"content\") or el.get(\"href\")\n",
    "                if src:\n",
    "                    if src.startswith(\"//\"):\n",
    "                        return \"https:\" + src\n",
    "                    if src.startswith(\"/\"):\n",
    "                        return urljoin(base_url, src)\n",
    "                    return src\n",
    "    except Exception as e:\n",
    "        logger.info(f\"get_og_image: {e}\")\n",
    "    return None\n",
    "\n",
    "def get_favicon_url(domain_url: str) -> str:\n",
    "    parsed = urlparse(domain_url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}/favicon.ico\"\n",
    "\n",
    "@st.cache_data(ttl=60*30, show_spinner=False)\n",
    "def fetch_article_text_and_image(url: str) -> Tuple[str, str]:\n",
    "    if not url:\n",
    "        return \"\", FALLBACK_IMG\n",
    "    html_text = fetch_page(url)\n",
    "    if not html_text:\n",
    "        return \"\", FALLBACK_IMG\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "        for tag in soup([\"script\", \"style\", \"noscript\", \"nav\", \"footer\", \"iframe\", \"form\"]):\n",
    "            tag.decompose()\n",
    "\n",
    "        candidates = []\n",
    "        selectors = [\n",
    "            \"article\", \"[role='main']\", \".article-body\", \".story-body\",\n",
    "            \".post-content\", \"main\", \".content\", \".entry-content\",\n",
    "            \"#article-body\", \".article-content\", \".story-content\",\n",
    "            \".news-content\", \".page-content\", \"body\",\n",
    "        ]\n",
    "        for sel in selectors:\n",
    "            for node in soup.select(sel):\n",
    "                text = node.get_text(separator=\" \", strip=True)\n",
    "                if len(text) > 100:\n",
    "                    candidates.append(text)\n",
    "        text = max(candidates, key=len) if candidates else soup.get_text(separator=\" \", strip=True)\n",
    "        text = _normalize(text)\n",
    "        if len(text) < 50:\n",
    "            text = \"\"\n",
    "\n",
    "        img = get_og_image(soup, url) or get_favicon_url(url) or FALLBACK_IMG\n",
    "        return text, img\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Used a fallback image for one article.\", f\"fetch_article_text_and_image EXC {url}: {e}\")\n",
    "        return \"\", FALLBACK_IMG\n",
    "\n",
    "# ========================= Relevance & Summary =========================\n",
    "def keyword_relevance(text: str, keywords: List[str]) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    if HAS_SK:\n",
    "        try:\n",
    "            vec = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "            X = vec.fit_transform([text, \" \".join(keywords)])\n",
    "            sim = cosine_similarity(X[0:1], X[1:2])[0][0]\n",
    "            return float(sim)\n",
    "        except Exception as e:\n",
    "            logger.info(f\"tfidf relevance fallback: {e}\")\n",
    "    tokens = re.findall(r\"[a-zA-Z']{3,}\", text.lower())\n",
    "    kwset = {k.lower() for k in keywords}\n",
    "    hits = sum(1 for t in tokens if t in kwset)\n",
    "    return hits / max(1, len(tokens))\n",
    "\n",
    "def simple_extractive_summary(text: str, n_sentences: int = 3, keywords: Optional[List[str]] = None) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    sents = re.split(r\"(?<=[\\.\\?\\!])\\s+\", text)\n",
    "    sents = [s for s in sents if 30 <= len(s) <= 400][:60]\n",
    "    if len(sents) <= n_sentences:\n",
    "        return \" \".join(sents)\n",
    "    if HAS_SK:\n",
    "        try:\n",
    "            vec = TfidfVectorizer(stop_words=\"english\", max_features=8000)\n",
    "            X = vec.fit_transform(sents)  # sparse\n",
    "            centroid = np.asarray(X.mean(axis=0)).ravel()\n",
    "            sims = cosine_similarity(X, centroid.reshape(1, -1)).ravel()\n",
    "            if keywords:\n",
    "                kw = [k.lower() for k in keywords]\n",
    "                boost = np.array([sum(1 for w in re.findall(r\"[a-z']+\", s.lower()) if w in kw) for s in sents], dtype=float)\n",
    "                sims = sims + 0.05 * boost\n",
    "            idx = sims.argsort()[-n_sentences:][::-1]\n",
    "            return \" \".join([sents[i] for i in idx])\n",
    "        except Exception as e:\n",
    "            logger.info(f\"summary fallback: {e}\")\n",
    "    return \" \".join(sents[:n_sentences])\n",
    "\n",
    "def classify_impact(text: str) -> List[str]:\n",
    "    tags = []\n",
    "    lower = text.lower()\n",
    "    for label, patterns in IMPACT_RULES.items():\n",
    "        try:\n",
    "            if any(re.search(p, lower) for p in patterns):\n",
    "                tags.append(label)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return list(dict.fromkeys(tags)) or [\"General\"]\n",
    "\n",
    "def parse_date(date_str: str) -> Optional[dt.datetime]:\n",
    "    try:\n",
    "        fmts = [\n",
    "            \"%Y-%m-%dT%H:%M:%S%z\", \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            \"%Y-%m-%d %H:%M:%S\", \"%a, %d %b %Y %H:%M:%S %z\",\n",
    "            \"%a, %d %b %Y %H:%M:%S %Z\", \"%Y-%m-%d\",\n",
    "            \"%d %b %Y\", \"%B %d, %Y\",\n",
    "        ]\n",
    "        for fmt in fmts:\n",
    "            try:\n",
    "                return dt.datetime.strptime(date_str, fmt)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        logger.info(f\"parse_date: {e}\")\n",
    "    return None\n",
    "\n",
    "# ========================= RSS/Atom parsing (no feedparser) =========================\n",
    "ATOM_NS = \"{http://www.w3.org/2005/Atom}\"\n",
    "\n",
    "def _text(elem: Optional[ET.Element]) -> str:\n",
    "    return _normalize(elem.text if elem is not None and elem.text else \"\")\n",
    "\n",
    "def _find(elem: ET.Element, tag: str) -> Optional[ET.Element]:\n",
    "    e = elem.find(tag)\n",
    "    if e is not None:\n",
    "        return e\n",
    "    if not tag.startswith(\"{\"):\n",
    "        e = elem.find(ATOM_NS + tag)\n",
    "    return e\n",
    "\n",
    "def _findall(elem: ET.Element, tag: str) -> List[ET.Element]:\n",
    "    return list(elem.findall(tag)) + list(elem.findall(ATOM_NS + tag))\n",
    "\n",
    "@st.cache_data(ttl=60*10, show_spinner=False)\n",
    "def fetch_feed_raw(url: str, timeout: int = 20) -> bytes:\n",
    "    try:\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0 OneAfricaPulse/1.0\",\n",
    "            \"Accept\": \"application/rss+xml, application/atom+xml, application/xml, text/xml, */*\",\n",
    "        }\n",
    "        r = get_session().get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        if r.status_code != 200:\n",
    "            soft_fail(\"Skipped a source that returned a non-200 response.\", f\"fetch_feed_raw {url} -> {r.status_code}\")\n",
    "        return r.content if r.status_code == 200 else (r.content or b\"\")\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Temporarily skipped one source due to connectivity.\", f\"fetch_feed_raw EXC {e}\")\n",
    "        return b\"\"\n",
    "\n",
    "def parse_feed_xml(content: bytes, base_url: str) -> List[Dict[str, str]]:\n",
    "    items: List[Dict[str, str]] = []\n",
    "    if not content:\n",
    "        return items\n",
    "    try:\n",
    "        sample = content.lstrip()[:16]\n",
    "        if not sample.startswith(b\"<\") and not sample.startswith(b\"\\xef\\xbb\\xbf<\"):\n",
    "            soft_fail(\"Skipped one feed that returned non-XML.\", f\"parse_feed_xml non-xml from {base_url}\")\n",
    "            return items\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        root = ET.fromstring(content)\n",
    "        channel = root.find(\"channel\")\n",
    "        if channel is not None:  # RSS\n",
    "            for it in channel.findall(\"item\"):\n",
    "                title = _text(_find(it, \"title\")) or \"(untitled)\"\n",
    "                link = _text(_find(it, \"link\"))\n",
    "                if not link:\n",
    "                    guid = _find(it, \"guid\")\n",
    "                    link = _text(guid)\n",
    "                if link and link.startswith(\"/\"):\n",
    "                    link = urljoin(base_url, link)\n",
    "                summary = _text(_find(it, \"description\"))\n",
    "                pub = _text(_find(it, \"pubDate\"))\n",
    "                if title or link:\n",
    "                    items.append({\"title\": title, \"link\": link, \"summary\": summary, \"published_raw\": pub})\n",
    "            return items\n",
    "\n",
    "        # Atom\n",
    "        for entry in _findall(root, \"entry\"):\n",
    "            title = _text(_find(entry, \"title\")) or \"(untitled)\"\n",
    "            link_el = _find(entry, \"link\")\n",
    "            link = \"\"\n",
    "            if link_el is not None:\n",
    "                link = link_el.attrib.get(\"href\", \"\") or _text(link_el)\n",
    "            if link and link.startswith(\"/\"):\n",
    "                link = urljoin(base_url, link)\n",
    "            summary = _text(_find(entry, \"summary\")) or _text(_find(entry, \"content\"))\n",
    "            pub = _text(_find(entry, \"updated\")) or _text(_find(entry, \"published\"))\n",
    "            if title or link:\n",
    "                items.append({\"title\": title, \"link\": link, \"summary\": summary, \"published_raw\": pub})\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Skipped one feed that had invalid XML.\", f\"parse_feed_xml EXC {e}\")\n",
    "        return items\n",
    "\n",
    "def validate_feed(url: str, ignore_recency_check: bool = False) -> Tuple[bool, str]:\n",
    "    try:\n",
    "        content = fetch_feed_raw(url)\n",
    "        items = parse_feed_xml(content, base_url=url)\n",
    "        if not items:\n",
    "            return False, \"No entries found\"\n",
    "        if ignore_recency_check:\n",
    "            return True, \"OK\"\n",
    "        now = dt.datetime.now(dt.timezone.utc)\n",
    "        for it in items[:10]:\n",
    "            d = parse_date(it.get(\"published_raw\", \"\") or \"\")\n",
    "            if d:\n",
    "                d = d.astimezone(dt.timezone.utc) if d.tzinfo else d.replace(tzinfo=dt.timezone.utc)\n",
    "                if (now - d).days <= 60:\n",
    "                    return True, \"OK\"\n",
    "        return False, \"No recent entries (‚â§60 days)\"\n",
    "    except Exception as ex:\n",
    "        soft_fail(\"Skipped one feed due to a validation issue.\", f\"validate_feed EXC {url}: {ex}\")\n",
    "        return False, \"Validation error\"\n",
    "\n",
    "def fetch_from_feed(url: str, start_date: dt.datetime, end_date: dt.datetime,\n",
    "                    force_fetch: bool, ignore_recency: bool) -> List[Dict[str, Any]]:\n",
    "    ok, status = validate_feed(url, ignore_recency_check=ignore_recency)\n",
    "    if not ok and not force_fetch:\n",
    "        soft_fail(f\"Skipped {urlparse(url).netloc} (feed not recent/valid).\", f\"validate -> {status}\")\n",
    "        return []\n",
    "    raw = fetch_feed_raw(url)\n",
    "    raw_items = parse_feed_xml(raw, base_url=url)\n",
    "\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    for e in raw_items:\n",
    "        title = _normalize(e.get(\"title\", \"\"))\n",
    "        link = e.get(\"link\", \"\")\n",
    "        summary = _normalize(e.get(\"summary\", \"\"))\n",
    "        published = None\n",
    "        if e.get(\"published_raw\"):\n",
    "            published = parse_date(e[\"published_raw\"])\n",
    "        if published:\n",
    "            published = published.astimezone(dt.timezone.utc) if published.tzinfo else published.replace(tzinfo=dt.timezone.utc)\n",
    "            if not (start_date <= published <= end_date):\n",
    "                continue\n",
    "            published_str = published.strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "        else:\n",
    "            published_str = \"Date unknown\"\n",
    "        items.append({\n",
    "            \"source\": urlparse(url).netloc,\n",
    "            \"title\": title,\n",
    "            \"link\": link,\n",
    "            \"published\": published_str,\n",
    "            \"summary\": summary,\n",
    "        })\n",
    "    return items\n",
    "\n",
    "# ========================= Newsdata.io (optional) =========================\n",
    "NEWSDATA_BASE = \"https://newsdata.io/api/1/latest\"\n",
    "\n",
    "@st.cache_data(ttl=60*10, show_spinner=False)\n",
    "def fetch_from_newsdata_cached(redacted_params: Dict[str, Any], max_pages: int) -> List[Dict[str, Any]]:\n",
    "    return []  # placeholder to keep cache signature consistent\n",
    "\n",
    "def fetch_from_newsdata_runtime(api_key: str, base_params: Dict[str, Any], max_pages: int) -> List[Dict[str, Any]]:\n",
    "    session = get_session()\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    pages = 0\n",
    "    next_page = None\n",
    "\n",
    "    params = dict(base_params)\n",
    "    params[\"apikey\"] = api_key\n",
    "\n",
    "    while pages < max_pages:\n",
    "        try:\n",
    "            q = dict(params)\n",
    "            if next_page:\n",
    "                q[\"page\"] = next_page\n",
    "            r = session.get(NEWSDATA_BASE, params=q, timeout=20)\n",
    "            if r.status_code != 200:\n",
    "                soft_fail(\"One API page was skipped (non-200).\", f\"newsdata {r.status_code} {r.text[:200]}\")\n",
    "                break\n",
    "            data = r.json()\n",
    "            results = data.get(\"results\") or data.get(\"articles\") or []\n",
    "            for a in results:\n",
    "                items.append(a)\n",
    "            next_page = data.get(\"nextPage\") or data.get(\"next_page\")\n",
    "            pages += 1\n",
    "            if not next_page:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            soft_fail(\"Temporarily skipped an API page due to connectivity.\", f\"newsdata EXC {e}\")\n",
    "            break\n",
    "    return items\n",
    "\n",
    "def fetch_from_newsdata(\n",
    "    api_key: str,\n",
    "    query: str,\n",
    "    start_date: dt.datetime,\n",
    "    end_date: dt.datetime,\n",
    "    language: Optional[str] = None,\n",
    "    country: Optional[str] = None,\n",
    "    category: Optional[str] = None,\n",
    "    max_pages: int = 2,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    if not api_key:\n",
    "        return []\n",
    "    redacted = {\"q\": query or \"\"}\n",
    "    if language: redacted[\"language\"] = language\n",
    "    if country: redacted[\"country\"] = country\n",
    "    if category: redacted[\"category\"] = category\n",
    "\n",
    "    _ = fetch_from_newsdata_cached(redacted, max_pages=max_pages)\n",
    "    items_raw = fetch_from_newsdata_runtime(api_key=api_key, base_params=redacted, max_pages=max_pages)\n",
    "\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    for a in items_raw:\n",
    "        try:\n",
    "            title = _normalize(a.get(\"title\", \"\"))\n",
    "            link = a.get(\"link\") or a.get(\"url\") or \"\"\n",
    "            source = a.get(\"source_id\") or a.get(\"source\") or \"newsdata.io\"\n",
    "            pub = a.get(\"pubDate\") or a.get(\"published_at\") or \"\"\n",
    "            desc = _normalize(a.get(\"description\", \"\")) or _normalize(a.get(\"content\", \"\"))\n",
    "\n",
    "            ok_date = True\n",
    "            published_str = \"Date unknown\"\n",
    "            if pub:\n",
    "                d = parse_date(pub)\n",
    "                if d:\n",
    "                    d = d.astimezone(dt.timezone.utc) if d.tzinfo else d.replace(tzinfo=dt.timezone.utc)\n",
    "                    ok_date = start_date <= d <= end_date\n",
    "                    published_str = d.strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "            if not ok_date:\n",
    "                continue\n",
    "            items.append({\n",
    "                \"source\": f\"{source} (newsdata.io)\",\n",
    "                \"title\": title or \"(untitled)\",\n",
    "                \"link\": link,\n",
    "                \"published\": published_str,\n",
    "                \"summary\": desc,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            soft_fail(\"Skipped one API item due to missing fields.\", f\"newsdata item EXC {e}\")\n",
    "            continue\n",
    "    return items\n",
    "\n",
    "# ========================= NEW: Grok-based X Fetch and Sentiment =========================\n",
    "def fetch_tweets_via_grok(query: str, lang: str, hours: int, max_results: int = 300) -> List[Dict[str, Any]]:\n",
    "    api_key = get_xai_api_key()\n",
    "    if not api_key:\n",
    "        soft_fail(\"XAI_API_KEY missing for Grok integration.\", \"Set XAI_API_KEY in .env or secrets.\")\n",
    "        return []\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant that can access real-time X (Twitter) posts.\n",
    "Fetch up to {max_results} recent posts matching this query: {query} lang:{lang}\n",
    "Look back no more than {hours} hours.\n",
    "Return ONLY a JSON array of objects with these fields for each post:\n",
    "- id: string\n",
    "- created_at: ISO format (YYYY-MM-DDTHH:MM:SSZ)\n",
    "- text: full text\n",
    "- lang: language code\n",
    "- retweets: integer\n",
    "- likes: integer\n",
    "- username: screen name\n",
    "- url: full URL like \"https://x.com/username/status/id\"\n",
    "No explanations or other text.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"grok-beta\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096,\n",
    "        )\n",
    "        raw_content = response.choices[0].message.content.strip()\n",
    "        tweets = json.loads(raw_content)\n",
    "        return tweets\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Grok tweet fetch failed.\", f\"EXC: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_tweet_sentiment_grok(tweets: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    api_key = get_xai_api_key()\n",
    "    if not api_key:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=\"https://api.x.ai/v1\")\n",
    "\n",
    "    texts = [t['text'] for t in tweets]\n",
    "    batch_size = 50  # to avoid token limits\n",
    "    compounds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        prompt = \"\"\"\n",
    "You are a sentiment analyst. For each text, return a compound score from -1 (negative) to +1 (positive).\n",
    "Return ONLY a JSON array of floats, no explanations.\n",
    "Texts:\n",
    "\"\"\" + \"\\n---\\n\".join(batch)\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"grok-beta\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.0,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "            raw = response.choices[0].message.content.strip()\n",
    "            scores = json.loads(raw)\n",
    "            compounds.extend(scores)\n",
    "        except Exception as e:\n",
    "            soft_fail(\"Grok sentiment analysis failed.\", f\"EXC: {e}\")\n",
    "            compounds.extend([0.0] * len(batch))\n",
    "\n",
    "    rows = []\n",
    "    for t, sc in zip(tweets, compounds):\n",
    "        label = \"Neutral\"\n",
    "        if sc >= 0.05:\n",
    "            label = \"Positive\"\n",
    "        elif sc <= -0.05:\n",
    "            label = \"Negative\"\n",
    "        rows.append({\n",
    "            \"created_at\": t.get(\"created_at\"),\n",
    "            \"text\": t.get(\"text\", \"\"),\n",
    "            \"compound\": sc,\n",
    "            \"label\": label,\n",
    "            \"retweets\": t.get(\"retweets\", 0),\n",
    "            \"likes\": t.get(\"likes\", 0),\n",
    "            \"username\": t.get(\"username\", \"\"),\n",
    "            \"url\": t.get(\"url\", \"\"),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "            df = df.sort_values(\"created_at\", ascending=False).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "# ========================= Social Sentiment (Twitter/X) =========================\n",
    "def get_sentiment_analyzer():\n",
    "    if not HAS_VADER:\n",
    "        return None\n",
    "    try:\n",
    "        return SentimentIntensityAnalyzer()\n",
    "    except Exception as e:\n",
    "        soft_fail(\"VADER analyzer unavailable.\", f\"vader EXC {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_tweet_sentiment(tweets: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    if not tweets:\n",
    "        return pd.DataFrame(columns=[\"created_at\",\"text\",\"compound\",\"label\",\"retweets\",\"likes\",\"username\",\"url\"])\n",
    "    sia = get_sentiment_analyzer()\n",
    "    rows = []\n",
    "    for t in tweets:\n",
    "        text = _normalize(t.get(\"text\",\"\"))\n",
    "        if not text:\n",
    "            continue\n",
    "        if sia:\n",
    "            sc = sia.polarity_scores(text).get(\"compound\", 0.0)\n",
    "        else:\n",
    "            sc = 0.0\n",
    "        label = \"Neutral\"\n",
    "        if sc >= 0.05:\n",
    "            label = \"Positive\"\n",
    "        elif sc <= -0.05:\n",
    "            label = \"Negative\"\n",
    "        rows.append({\n",
    "            \"created_at\": t.get(\"created_at\"),\n",
    "            \"text\": text,\n",
    "            \"compound\": sc,\n",
    "            \"label\": label,\n",
    "            \"retweets\": t.get(\"retweets\", 0),\n",
    "            \"likes\": t.get(\"likes\", 0),\n",
    "            \"username\": t.get(\"username\", \"\"),\n",
    "            \"url\": t.get(\"url\", \"\"),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        try:\n",
    "            df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "            df = df.sort_values(\"created_at\", ascending=False).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def summarize_sentiment(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if df is None or df.empty:\n",
    "        return {\"n\": 0, \"mean_compound\": 0.0, \"share_pos\": 0.0, \"share_neu\": 0.0, \"share_neg\": 0.0}\n",
    "    n = len(df)\n",
    "    mean_c = float(df[\"compound\"].mean())\n",
    "    share_pos = float((df[\"label\"] == \"Positive\").mean())\n",
    "    share_neu = float((df[\"label\"] == \"Neutral\").mean())\n",
    "    share_neg = float((df[\"label\"] == \"Negative\").mean())\n",
    "    return {\"n\": n, \"mean_compound\": mean_c, \"share_pos\": share_pos, \"share_neu\": share_neu, \"share_neg\": share_neg}\n",
    "\n",
    "# ========================= NEW: Word (.docx) Builder =========================\n",
    "def _set_heading_style(p, size=16, bold=True):\n",
    "    run = p.runs[0] if p.runs else p.add_run()\n",
    "    run.font.size = Pt(size)\n",
    "    run.font.bold = bold\n",
    "\n",
    "def _add_kv_para(doc: Document, key: str, val: str):\n",
    "    p = doc.add_paragraph()\n",
    "    p.add_run(f\"{key}: \").bold = True\n",
    "    p.add_run(val or \"-\")\n",
    "\n",
    "def build_results_docx(\n",
    "    app_name: str,\n",
    "    tagline: str,\n",
    "    quote: str,\n",
    "    results_df: pd.DataFrame,\n",
    "    digest_md: str,\n",
    "    params: Dict[str, Any],\n",
    "    sent_summary: Dict[str, Any] | None = None\n",
    ") -> bytes:\n",
    "    \"\"\"\n",
    "    Returns a bytes object for a .docx containing:\n",
    "    - cover info (title, tagline, quote, timestamp, filters)\n",
    "    - results table (Source, Published, Title, Relevance, Impact)\n",
    "    - per-article summaries\n",
    "    - optional Social Sentiment metrics\n",
    "    - Daily Digest (Markdown flattened)\n",
    "    \"\"\"\n",
    "    doc = Document()\n",
    "\n",
    "    # --- Cover / Title ---\n",
    "    title = doc.add_paragraph(app_name)\n",
    "    _set_heading_style(title, size=22, bold=True)\n",
    "    sub = doc.add_paragraph(tagline)\n",
    "    _set_heading_style(sub, size=12, bold=False)\n",
    "    doc.add_paragraph(quote)\n",
    "\n",
    "    # Timestamp + window\n",
    "    ts = dt.datetime.now(dt.timezone.utc).strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "    _add_kv_para(doc, \"Generated\", ts)\n",
    "    try:\n",
    "        _add_kv_para(\n",
    "            doc, \"Date Window\",\n",
    "            f\"{params.get('start_date').strftime('%Y-%m-%d')} ‚Üí {params.get('end_date').strftime('%Y-%m-%d')}\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pass\n",
    "    _add_kv_para(doc, \"Min Relevance\", f\"{params.get('min_relevance',0):.2f}\")\n",
    "    _add_kv_para(doc, \"Keywords\", \", \".join(params.get('keywords', [])) or \"-\")\n",
    "\n",
    "    doc.add_paragraph().add_run(\" \").add_break()  # spacer\n",
    "\n",
    "    # --- Results summary table (compact) ---\n",
    "    doc.add_paragraph(\"Results Overview\").runs[0].bold = True\n",
    "    if results_df is None or results_df.empty:\n",
    "        doc.add_paragraph(\"No relevant items found for the selected period.\")\n",
    "    else:\n",
    "        show_cols = [\"source\", \"published\", \"title\", \"relevance\", \"impact\"]\n",
    "        tmp = results_df[show_cols].copy()\n",
    "        tmp[\"impact\"] = tmp[\"impact\"].apply(lambda x: \", \".join(x) if isinstance(x, list) else str(x))\n",
    "        tmp[\"relevance\"] = tmp[\"relevance\"].apply(lambda r: f\"{r:.0%}\")\n",
    "\n",
    "        table = doc.add_table(rows=1, cols=len(show_cols))\n",
    "        hdr_cells = table.rows[0].cells\n",
    "        for i, c in enumerate([\"Source\", \"Published\", \"Title\", \"Relevance\", \"Impact\"]):\n",
    "            hdr_cells[i].text = c\n",
    "\n",
    "        for _, row in tmp.head(50).iterrows():\n",
    "            cells = table.add_row().cells\n",
    "            cells[0].text = str(row[\"source\"])\n",
    "            cells[1].text = str(row[\"published\"])\n",
    "            cells[2].text = str(row[\"title\"])\n",
    "            cells[3].text = str(row[\"relevance\"])\n",
    "            cells[4].text = str(row[\"impact\"])\n",
    "\n",
    "    doc.add_paragraph().add_run(\" \").add_break()  # spacer\n",
    "\n",
    "    # --- Per-article blocks with summaries ---\n",
    "    if results_df is not None and not results_df.empty:\n",
    "        h = doc.add_paragraph(\"Articles & Auto-Summaries\")\n",
    "        _set_heading_style(h, size=14, bold=True)\n",
    "\n",
    "        for i, row in results_df.iterrows():\n",
    "            doc.add_paragraph().add_run(\" \").add_break()  # spacer between cards\n",
    "            p = doc.add_paragraph(f\"{i+1}. {row['title']}\")\n",
    "            _set_heading_style(p, size=12, bold=True)\n",
    "\n",
    "            meta = doc.add_paragraph()\n",
    "            meta.add_run(\"Source: \").bold = True\n",
    "            meta.add_run(str(row[\"source\"]))\n",
    "            meta.add_run(\"   |   Published: \").bold = True\n",
    "            meta.add_run(str(row[\"published\"]))\n",
    "            meta.add_run(\"   |   Relevance: \").bold = True\n",
    "            try:\n",
    "                meta.add_run(f\"{row['relevance']:.0%}\")\n",
    "            except Exception:\n",
    "                meta.add_run(str(row[\"relevance\"]))\n",
    "\n",
    "            imp = \", \".join(row[\"impact\"]) if isinstance(row[\"impact\"], list) else str(row[\"impact\"])\n",
    "            impp = doc.add_paragraph()\n",
    "            impp.add_run(\"Impact: \").bold = True\n",
    "            impp.add_run(imp)\n",
    "\n",
    "            sump = doc.add_paragraph()\n",
    "            sump.add_run(\"Summary: \").bold = True\n",
    "            sump.add_run(row.get(\"auto_summary\") or \"\")\n",
    "\n",
    "            linkp = doc.add_paragraph()\n",
    "            linkp.add_run(\"Link: \").bold = True\n",
    "            linkp.add_run(row.get(\"link\") or \"\")\n",
    "\n",
    "    # --- Social Sentiment (optional) ---\n",
    "    if sent_summary and isinstance(sent_summary, dict) and (sent_summary.get(\"n\", 0) > 0):\n",
    "        doc.add_paragraph().add_run(\" \").add_break()\n",
    "        h2 = doc.add_paragraph(\"Twitter/X Sentiment Snapshot\")\n",
    "        _set_heading_style(h2, size=14, bold=True)\n",
    "        _add_kv_para(doc, \"Tweets analyzed\", str(int(sent_summary.get(\"n\", 0))))\n",
    "        _add_kv_para(doc, \"Mean (compound)\", f\"{sent_summary.get('mean_compound', 0.0):+.3f}\")\n",
    "        _add_kv_para(doc, \"Positive\", f\"{100*sent_summary.get('share_pos',0.0):.1f}%\")\n",
    "        _add_kv_para(doc, \"Neutral\", f\"{100*sent_summary.get('share_neu',0.0):.1f}%\")\n",
    "        _add_kv_para(doc, \"Negative\", f\"{100*sent_summary.get('share_neg',0.0):.1f}%\")\n",
    "\n",
    "    # --- Daily Digest (Markdown text flattened) ---\n",
    "    if digest_md:\n",
    "        doc.add_paragraph().add_run(\" \").add_break()\n",
    "        h3 = doc.add_paragraph(\"Daily Digest\")\n",
    "        _set_heading_style(h3, size=14, bold=True)\n",
    "\n",
    "        lines = (digest_md or \"\").splitlines()\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith(\"#\"):\n",
    "                txt = line.lstrip(\"# \").strip()\n",
    "                p = doc.add_paragraph(txt)\n",
    "                _set_heading_style(p, size=12, bold=True)\n",
    "            elif line.startswith((\"- \", \"* \")):\n",
    "                p = doc.add_paragraph(line[2:])\n",
    "                p_format = p.paragraph_format\n",
    "                p_format.left_indent = Inches(0.25)\n",
    "            elif line.strip() == \"---\":\n",
    "                doc.add_paragraph().add_run(\"‚Äî\" * 20)\n",
    "            else:\n",
    "                if line.strip():\n",
    "                    doc.add_paragraph(line)\n",
    "\n",
    "    buf = BytesIO()\n",
    "    doc.save(buf)\n",
    "    buf.seek(0)\n",
    "    return buf.getvalue()\n",
    "\n",
    "# ========================= UI Helpers =========================\n",
    "st.set_page_config(page_title=APP_NAME, page_icon=\"üåç\", layout=\"wide\", initial_sidebar_state=\"expanded\")\n",
    "st.markdown(CARD_CSS, unsafe_allow_html=True)\n",
    "\n",
    "def make_digest(df: pd.DataFrame, top_k: int = 12) -> str:\n",
    "    header = f\"# {APP_NAME} ‚Äî Daily Digest\\n\\n*{TAGLINE}*\\n\\n> {QUOTE}\\n\\n\"\n",
    "    if df.empty:\n",
    "        return header + \"_No relevant items found for the selected period._\"\n",
    "    parts = [header, f\"**Date:** {dt.datetime.now(dt.timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}\\n\\n\"]\n",
    "    for _, row in df.head(top_k).iterrows():\n",
    "        impact = \", \".join(row[\"impact\"])\n",
    "        parts.append(\n",
    "            f\"### {row['title']}\\n\"\n",
    "            f\"- **Source:** {row['source']}  \\n\"\n",
    "            f\"- **Published:** {row['published']}  \\n\"\n",
    "            f\"- **Relevance:** {row['relevance']:.3f}  \\n\"\n",
    "            f\"- **Impact:** {impact}  \\n\"\n",
    "            f\"- **Summary:** {row['auto_summary']}\\n\"\n",
    "            f\"[Read more]({row['link']})\\n\\n---\\n\"\n",
    "        )\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "# ======= HERO =======\n",
    "with st.container():\n",
    "    st.markdown(f\"\"\"\n",
    "    <div class=\"hero\">\n",
    "        <div class=\"pill\">üåç One Africa Market Pulse</div>\n",
    "        <h1>{TAGLINE}</h1>\n",
    "        <p>{QUOTE}</p>\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# ======= ACTION BAR =======\n",
    "st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "act_b1, act_b2, act_b3 = st.columns([1, 1, 1])\n",
    "with act_b1:\n",
    "    run_btn = st.button(\"üöÄ Scan Now\", use_container_width=True, key=\"run_main\")\n",
    "with act_b2:\n",
    "    if st.button(\"‚ôªÔ∏è Reset\", use_container_width=True, key=\"reset_main\"):\n",
    "        for k in list(st.session_state.keys()):\n",
    "            if k.startswith((\"results_\", \"ai_\", \"chat_\", \"cfg_\", \"last_scan_\", \"filters_\", \"sent_\")):\n",
    "                del st.session_state[k]\n",
    "        st.rerun()\n",
    "with act_b3:\n",
    "    if st.button(\"üîÑ Refresh View\", use_container_width=True, key=\"refresh_view\"):\n",
    "        st.rerun()\n",
    "\n",
    "# ======= Quick Analyze by URL (LLM-only) =======\n",
    "st.markdown(\"### üîó Quick Analyze by URL (LLM)\")\n",
    "qa_col1, qa_col2 = st.columns([4,1])\n",
    "with qa_col1:\n",
    "    quick_url = st.text_input(\"Paste any article URL\", value=\"\", placeholder=\"https://example.com/article\")\n",
    "with qa_col2:\n",
    "    run_quick = st.button(\"Analyze\", use_container_width=True, key=\"an_quick\")\n",
    "\n",
    "# ======= CHAT ASSISTANT =======\n",
    "st.markdown(\"### ü§ñ Chat Assistant\")\n",
    "st.caption(\"Ask follow-ups, draft digests, or generate summaries. Uses your `.env`/Secrets OPENAI_API_KEY if available.\")\n",
    "\n",
    "def init_chat_state():\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = [\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are a crisp market-intelligence assistant for West African tree crops \"\n",
    "                \"(cashew, shea, cocoa, palm kernel). Be concise, cite assumptions, and suggest \"\n",
    "                \"actionable next steps. If asked to summarize a table, write bullet points.\"\n",
    "            )}\n",
    "        ]\n",
    "\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    init_chat_state()\n",
    "\n",
    "def have_openai():\n",
    "    return OPENAI_OK and bool(get_openai_api_key())\n",
    "\n",
    "# ---- Robust client (respects OPENAI_BASE_URL if set) ----\n",
    "def get_openai_client():\n",
    "    try:\n",
    "        api_key = get_openai_api_key()\n",
    "        if not api_key:\n",
    "            return None\n",
    "        base_url = os.environ.get(\"OPENAI_BASE_URL\", \"\").strip()\n",
    "        if base_url:\n",
    "            return OpenAI(api_key=api_key, base_url=base_url)\n",
    "        return OpenAI(api_key=api_key)  # official endpoint\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"OpenAI client init failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_assistant_reply(messages, temperature: float = 0.4):\n",
    "    if not have_openai():\n",
    "        return None, False\n",
    "    client = get_openai_client()\n",
    "    if client is None:\n",
    "        return None, False\n",
    "\n",
    "    model_candidates = [\n",
    "        \"gpt-4o-mini\",\n",
    "        \"gpt-4o\",\n",
    "        \"gpt-4.1-mini\",\n",
    "        \"gpt-3.5-turbo-0125\",\n",
    "    ]\n",
    "\n",
    "    last_err = None\n",
    "    for model in model_candidates:\n",
    "        try:\n",
    "            stream = client.chat.completions.create(\n",
    "                model=model, messages=messages, stream=True, temperature=temperature\n",
    "            )\n",
    "            chunks = []\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                placeholder = st.empty()\n",
    "                buf = \"\"\n",
    "                for chunk in stream:\n",
    "                    delta = chunk.choices[0].delta.content or \"\"\n",
    "                    if delta:\n",
    "                        buf += delta\n",
    "                        placeholder.markdown(buf)\n",
    "                chunks.append(buf)\n",
    "            reply = \"\".join(chunks).strip()\n",
    "            if reply:\n",
    "                return reply, True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"OpenAI streaming failed on {model}: {e}\")\n",
    "            last_err = e\n",
    "\n",
    "        try:\n",
    "            comp = client.chat.completions.create(\n",
    "                model=model, messages=messages, temperature=temperature\n",
    "            )\n",
    "            reply = (comp.choices[0].message.content or \"\").strip()\n",
    "            if reply:\n",
    "                return reply, False\n",
    "        except Exception as e2:\n",
    "            logger.warning(f\"OpenAI non-streaming failed on {model}: {e2}\")\n",
    "            last_err = e2\n",
    "            continue\n",
    "\n",
    "    soft_fail(\"Assistant is temporarily unavailable.\", f\"OpenAI failures: {last_err}\")\n",
    "    return None, False\n",
    "\n",
    "# Render prior chat (omit system)\n",
    "for m in st.session_state.chat_history:\n",
    "    if m[\"role\"] == \"system\":\n",
    "        continue\n",
    "    with st.chat_message(m[\"role\"]):\n",
    "        st.markdown(m[\"content\"])\n",
    "\n",
    "# Chat input\n",
    "user_prompt = st.chat_input(\"Type your message...\")\n",
    "if user_prompt:\n",
    "    st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_prompt)\n",
    "\n",
    "    if not have_openai():\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.warning(\"No `OPENAI_API_KEY` found (in `.env` or Streamlit Secrets). Add it and press **Reset**.\")\n",
    "    else:\n",
    "        reply, _streamed = generate_assistant_reply(st.session_state.chat_history)\n",
    "        if reply:\n",
    "            if not _streamed:\n",
    "                with st.chat_message(\"assistant\"):\n",
    "                    st.markdown(reply)\n",
    "            st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        else:\n",
    "            with st.chat_message(\"assistant\"):\n",
    "                st.error(\"The assistant is temporarily unavailable. Please try again in a moment.\")\n",
    "\n",
    "# ========================= Diagnostics =========================\n",
    "with st.sidebar:\n",
    "    with st.expander(\"üß™ Diagnostics\", expanded=False):\n",
    "        st.write(\"Check your AI setup quickly.\")\n",
    "        st.write(f\"OPENAI package installed: **{OPENAI_OK}**\")\n",
    "        key_present = \"Yes\" if get_openai_api_key() else \"No\"\n",
    "        st.write(f\"OPENAI_API_KEY present: **{key_present}**\")\n",
    "        st.write(f\"OPENAI_BASE_URL: **{os.environ.get('OPENAI_BASE_URL','(not set)')}**\")\n",
    "        tw_present = \"Yes\" if get_twitter_bearer() else \"No\"\n",
    "        st.write(f\"TWITTER_BEARER_TOKEN present: **{tw_present}**\")\n",
    "        xai_present = \"Yes\" if get_xai_api_key() else \"No\"\n",
    "        st.write(f\"XAI_API_KEY present: **{xai_present}**\")\n",
    "        if st.button(\"Run AI self-test\"):\n",
    "            if not have_openai():\n",
    "                st.error(\"No OPENAI_API_KEY or package not installed.\")\n",
    "            else:\n",
    "                client = get_openai_client()\n",
    "                if client is None:\n",
    "                    st.error(\"Could not initialize OpenAI client (see logs).\")\n",
    "                else:\n",
    "                    try:\n",
    "                        resp = client.chat.completions.create(\n",
    "                            model=\"gpt-4o-mini\", temperature=0,\n",
    "                            messages=[{\"role\":\"system\",\"content\":\"You are a tester.\"},\n",
    "                                      {\"role\":\"user\",\"content\":\"Reply with the single word: OK\"}]\n",
    "                        )\n",
    "                        msg = (resp.choices[0].message.content or \"\").strip()\n",
    "                        st.success(f\"LLM replied: {msg[:200]}\")\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Model call failed: {e}\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "        st.write(\"**Grok connectivity quick test**\")\n",
    "        if st.button(\"Test Grok\"):\n",
    "            api_key = get_xai_api_key()\n",
    "            if not api_key:\n",
    "                st.error(\"No XAI_API_KEY found. Set it in .env or secrets.toml.\")\n",
    "            else:\n",
    "                client = OpenAI(api_key=api_key, base_url=\"https://api.x.ai/v1\")\n",
    "                try:\n",
    "                    resp = client.chat.completions.create(\n",
    "                        model=\"grok-beta\",\n",
    "                        messages=[{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
    "                        temperature=0.0,\n",
    "                        max_tokens=10\n",
    "                    )\n",
    "                    msg = (resp.choices[0].message.content or \"\").strip()\n",
    "                    st.success(f\"Grok replied: {msg}\")\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Grok call failed: {e}\")\n",
    "\n",
    "# ========================= LLM-only Article Analysis =========================\n",
    "@st.cache_data(ttl=30*60, show_spinner=False)\n",
    "def _llm_analyze_article_cached(model: str, title: str, body: str, tags: List[str]) -> str:\n",
    "    client = get_openai_client()\n",
    "    if client is None:\n",
    "        return \"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a market-intelligence analyst focused on tree crop commodities in the world\n",
    "(cashew, shea, cocoa, palm kernel), logistics, and FX.\n",
    "\n",
    "Analyze the ARTICLE and produce a concise, executive-ready brief. Use short, punchy bullets\n",
    "where appropriate and provide concrete, actionable guidance. Avoid fluff.\n",
    "\n",
    "ARTICLE TITLE:\n",
    "{title[:400]}\n",
    "\n",
    "ARTICLE BODY (may be partial):\n",
    "{body[:7000]}\n",
    "\n",
    "HEURISTIC TAGS PROVIDED BY UI (may be incomplete):\n",
    "{\", \".join(tags) if tags else \"General\"}\n",
    "\n",
    "Return your analysis in EXACTLY these sections with clear headings:\n",
    "1) WHAT THE ARTICLE MEANS ‚Äî 2‚Äì3 sentence synthesis\n",
    "2) KEY INSIGHTS ‚Äî 3‚Äì6 bullets with the most important takeaways\n",
    "3) MARKET IMPACT ‚Äî specific effects on supply/demand, prices, logistics, FX; note direction & magnitude if possible\n",
    "4) BUSINESS OPPORTUNITIES ‚Äî 3‚Äì6 concrete moves we could make now (be specific)\n",
    "5) RISK FACTORS ‚Äî 3‚Äì5 concise bullets (operational, financial/FX, regulatory)\n",
    "6) ACTIONABLE RECOMMENDATIONS ‚Äî 3‚Äì5 steps with owners or thresholds where relevant\n",
    "7) TIME HORIZON ‚Äî near-term (0‚Äì3m) / medium (3‚Äì12m) / long (12m+)\n",
    "8) CONFIDENCE ‚Äî High/Medium/Low and why\n",
    "\n",
    "Constraints:\n",
    "- Keep it pragmatic and West-Africa oriented.\n",
    "- If information is uncertain, say so explicitly and suggest a verification step.\n",
    "\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model, temperature=0.3,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Be precise, actionable, and bias towards decisions and thresholds.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "def analyze_with_llm(title: str, body: str, tags: List[str]) -> str:\n",
    "    if not have_openai():\n",
    "        return \"\"\n",
    "    model_candidates = [\"gpt-4o-mini\",\"gpt-4o\",\"gpt-4.1-mini\",\"gpt-3.5-turbo-0125\"]\n",
    "    for m in model_candidates:\n",
    "        try:\n",
    "            out = _llm_analyze_article_cached(m, title, body, tags)\n",
    "            if out:\n",
    "                return out\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"LLM analyze failed on {m}: {e}\")\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "# ========================= SINGLE COLLAPSIBLE CONFIG PANEL ====================\n",
    "with st.sidebar:\n",
    "    with st.expander(\"‚öôÔ∏è Configurations\", expanded=False):\n",
    "        st.header(\"Settings\")\n",
    "\n",
    "        # üì∞ RSS/Atom Sources\n",
    "        st.subheader(\"üì∞ RSS/Atom Sources\")\n",
    "        chosen_sources: List[str] = []\n",
    "        for name, url in DEFAULT_SOURCES.items():\n",
    "            if st.checkbox(name, value=True, key=f\"src_{name}\"):\n",
    "                chosen_sources.append(url)\n",
    "        if st.button(\"üîÑ Check Feeds\", key=\"check_feeds\"):\n",
    "            for name, url in DEFAULT_SOURCES.items():\n",
    "                ok, status = validate_feed(url, ignore_recency_check=True)\n",
    "                st.write(f\"{'‚úÖ' if ok else '‚ùå'} {name}: {status}\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üß© Newsdata.io (optional)\n",
    "        st.subheader(\"üß© Newsdata.io (optional)\")\n",
    "        st.caption(\"Merge API headlines with the same scoring & summaries.\")\n",
    "        use_newsdata = st.checkbox(\"Use Newsdata.io\", value=True, key=\"use_nd\")\n",
    "\n",
    "        auto_key = get_newsdata_api_key()\n",
    "        override = st.checkbox(\"Temporarily override API key (not saved)\", value=False, key=\"nd_override\")\n",
    "        tmp_key = st.text_input(\"Enter API key\", type=\"password\", key=\"nd_key_input\") if override else \"\"\n",
    "        newsdata_key = (tmp_key or auto_key).strip()\n",
    "\n",
    "        if use_newsdata:\n",
    "            if newsdata_key:\n",
    "                st.success(\"Using secured API key.\")\n",
    "            else:\n",
    "                st.warning(\"No API key found. Add NEWSDATA_API_KEY to `.env`/Secrets, or use a temporary override.\")\n",
    "\n",
    "        newsdata_query = st.text_input(\"Query\", value=\"tree crop commodities\", key=\"nd_query\")\n",
    "        c1, c2, c3 = st.columns(3)\n",
    "        with c1: nd_language = st.text_input(\"Language (e.g., en, fr)\", value=\"\", key=\"nd_lang\")\n",
    "        with c2: nd_country = st.text_input(\"Country (e.g., gh, ng, ci)\", value=\"\", key=\"nd_cty\")\n",
    "        with c3: nd_category = st.text_input(\"Category (e.g., business)\", value=\"\", key=\"nd_cat\")\n",
    "        nd_pages = st.number_input(\"Newsdata pages\", min_value=1, max_value=10, value=2, step=1, key=\"nd_pages\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üìÖ Date Range\n",
    "        st.subheader(\"üìÖ Date Range\")\n",
    "        mode = st.radio(\"Mode\", [\"Quick Select\", \"Custom\"], horizontal=True, key=\"date_mode\")\n",
    "        if mode == \"Quick Select\":\n",
    "            quick = {\"Last 24 Hours\": 1, \"Last 3 Days\": 3, \"Last Week\": 7, \"Last 2 Weeks\": 14, \"Last Month\": 30}\n",
    "            sel = st.selectbox(\"Window\", list(quick.keys()), index=2, key=\"date_win\")\n",
    "            end_date = dt.datetime.now(dt.timezone.utc)\n",
    "            start_date = end_date - dt.timedelta(days=quick[sel])\n",
    "        else:\n",
    "            d1, d2 = st.columns(2)\n",
    "            with d1: sd = st.date_input(\"Start\", value=dt.date.today() - dt.timedelta(days=7), key=\"start_date\")\n",
    "            with d2: ed = st.date_input(\"End\", value=dt.date.today(), key=\"end_date\")\n",
    "            start_date = dt.datetime.combine(sd, dt.time.min, tzinfo=dt.timezone.utc)\n",
    "            end_date = dt.datetime.combine(ed, dt.time.max, tzinfo=dt.timezone.utc)\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üîç Keywords & Filters\n",
    "        st.subheader(\"üîç Keywords & Filters\")\n",
    "        custom_kw = st.text_area(\"Keywords (comma-separated)\", \", \".join(DEFAULT_KEYWORDS), height=100, key=\"kw_text\")\n",
    "        keywords = [k.strip() for k in custom_kw.split(\",\") if k.strip()]\n",
    "        min_relevance = st.number_input(\"Min relevance (0.00‚Äì1.00)\", min_value=0.0, max_value=1.0, value=0.05, step=0.01, format=\"%.2f\", key=\"min_rel\")\n",
    "        per_source_cap = st.number_input(\"Max articles per source\", min_value=1, max_value=200, value=10, step=1, key=\"cap\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üìù Content Settings\n",
    "        st.subheader(\"üìù Content Settings\")\n",
    "        n_sent = st.number_input(\"Sentences per summary\", min_value=2, max_value=10, value=3, step=1, key=\"n_sent\")\n",
    "        top_k = st.number_input(\"Digest: top items\", min_value=5, max_value=100, value=12, step=1, key=\"top_k\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üê¶ Social Sentiment (Twitter/X)\n",
    "        st.subheader(\"üê¶ Social Sentiment (Twitter/X)\")\n",
    "        enable_social = st.checkbox(\"Enable Twitter/X sentiment\", value=True, key=\"enable_social\")\n",
    "        sentiment_method = st.radio(\"Sentiment Method\", [\"Grok (xAI API)\", \"Ollama (Local)\"], key=\"sentiment_method\")\n",
    "        default_query = \" OR \".join([kw for kw in keywords if \" \" not in kw][:6]) or \"cashew OR shea OR cocoa\"\n",
    "        tw_query = st.text_input(\"Twitter search query\", value=default_query, help=\"Example: cashew OR shea OR cocoa\", key=\"tw_query\")\n",
    "        col_t1, col_t2, col_t3 = st.columns(3)\n",
    "        with col_t1:\n",
    "            tw_hours = st.number_input(\"Lookback hours\", min_value=6, max_value=720, value=72, step=6, key=\"tw_hours\")\n",
    "        with col_t2:\n",
    "            tw_lang = st.text_input(\"Language filter (e.g., en, fr) or blank\", value=\"en\", key=\"tw_lang\")\n",
    "        with col_t3:\n",
    "            tw_max = st.number_input(\"Max tweets\", min_value=10, max_value=1000, value=300, step=50, key=\"tw_max\")\n",
    "\n",
    "        st.caption(\"Tip: For Grok, set XAI_API_KEY. For Ollama, install locally and run 'ollama serve' with a model like llama3.\")\n",
    "\n",
    "        st.markdown(\"---\")\n",
    "\n",
    "        # üõ°Ô∏è Resilience\n",
    "        st.subheader(\"üõ°Ô∏è Resilience\")\n",
    "        force_fetch = st.checkbox(\"‚ö° Force RSS fetch if validation fails\", value=True, key=\"force\")\n",
    "        ignore_recency = st.checkbox(\"üïí Ignore RSS recency check\", value=True, key=\"ignore_recent\")\n",
    "        dedupe_across_sources = st.checkbox(\"üßπ Deduplicate across sources\", value=True, key=\"dedupe\")\n",
    "\n",
    "# Build a single immutable dict of current config (no recompute unless Scan Now)\n",
    "current_params = {\n",
    "    \"chosen_sources\": chosen_sources[:],\n",
    "    \"use_newsdata\": bool(use_newsdata),\n",
    "    \"newsdata_key\": newsdata_key,\n",
    "    \"newsdata_query\": newsdata_query,\n",
    "    \"nd_language\": nd_language,\n",
    "    \"nd_country\": nd_country,\n",
    "    \"nd_category\": nd_category,\n",
    "    \"nd_pages\": int(nd_pages),\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"keywords\": keywords[:],\n",
    "    \"min_relevance\": float(min_relevance),\n",
    "    \"per_source_cap\": int(per_source_cap),\n",
    "    \"n_sent\": int(n_sent),\n",
    "    \"top_k\": int(top_k),\n",
    "    \"force_fetch\": bool(force_fetch),\n",
    "    \"ignore_recency\": bool(ignore_recency),\n",
    "    \"dedupe\": bool(dedupe_across_sources),\n",
    "    # Social\n",
    "    \"enable_social\": bool(enable_social),\n",
    "    \"sentiment_method\": sentiment_method,\n",
    "    \"tw_query\": tw_query,\n",
    "    \"tw_hours\": int(tw_hours),\n",
    "    \"tw_lang\": tw_lang.strip(),\n",
    "    \"tw_max\": int(tw_max),\n",
    "}\n",
    "\n",
    "# ---- Durable stores (persist across reruns) ----\n",
    "ss_get(\"results_df\", None)\n",
    "ss_get(\"results_digest_md\", \"\")\n",
    "ss_get(\"ai_analyses\", {})\n",
    "ss_get(\"last_scan_params\", {})\n",
    "ss_get(\"filters_impact\", [])\n",
    "ss_get(\"filters_source\", [])\n",
    "ss_get(\"sent_df\", None)\n",
    "ss_get(\"sent_summary\", {})\n",
    "\n",
    "# Quick Analyze trigger (uses LLM only)\n",
    "if run_quick:\n",
    "    if not have_openai():\n",
    "        st.warning(\"Add an `OPENAI_API_KEY` to use AI analysis.\")\n",
    "    elif not quick_url:\n",
    "        st.info(\"Please paste a valid URL.\")\n",
    "    else:\n",
    "        with st.spinner(\"Fetching and analyzing...\"):\n",
    "            text, img = fetch_article_text_and_image(quick_url)\n",
    "            if not text:\n",
    "                st.error(\"Could not extract article text from this URL.\")\n",
    "            else:\n",
    "                title_guess = text.split(\".\")[0][:140] if text else quick_url\n",
    "                tags = classify_impact(text)\n",
    "                md = analyze_with_llm(title_guess, text, tags)\n",
    "                if not md:\n",
    "                    st.error(\"AI analysis failed. Please try again.\")\n",
    "                else:\n",
    "                    st.image(img, use_column_width=True)\n",
    "                    st.markdown(f\"**Source:** {urlparse(quick_url).netloc}\")\n",
    "                    st.markdown(md)\n",
    "\n",
    "# ========================= Processing =========================\n",
    "def hash_key(*parts) -> str:\n",
    "    return hashlib.md5((\"||\".join([p or \"\" for p in parts])).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def process_rows(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"source\",\"published\",\"title\",\"relevance\",\"impact\",\"auto_summary\",\"link\",\"image\"])\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for r in rows:\n",
    "        key = hash_key(r.get(\"title\",\"\"), r.get(\"link\",\"\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        cleaned.append(r)\n",
    "    df = pd.DataFrame(cleaned)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"source\",\"published\",\"title\",\"relevance\",\"impact\",\"auto_summary\",\"link\",\"image\"])\n",
    "    return df.sort_values(\"relevance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def enrich(entry: Dict[str, Any], keywords: List[str], min_relevance: float, n_sent: int) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        article_text, image_url = fetch_article_text_and_image(entry.get(\"link\",\"\"))\n",
    "        base = entry.get(\"summary\") or \"\"\n",
    "        body = article_text if len(article_text) > len(base) else base\n",
    "\n",
    "        rel = keyword_relevance(\" \".join([entry.get(\"title\",\"\"), body]), keywords)\n",
    "        if rel < min_relevance:\n",
    "            return None\n",
    "        summary = simple_extractive_summary(body, n_sentences=n_sent, keywords=keywords)\n",
    "        impacts = classify_impact(\" \".join([entry.get(\"title\",\"\"), body])) or [\"General\"]\n",
    "\n",
    "        return {\n",
    "            \"source\": entry.get(\"source\",\"\"),\n",
    "            \"title\": entry.get(\"title\",\"(untitled)\"),\n",
    "            \"link\": entry.get(\"link\",\"\"),\n",
    "            \"published\": entry.get(\"published\",\"Date unknown\"),\n",
    "            \"relevance\": float(rel),\n",
    "            \"impact\": impacts,\n",
    "            \"auto_summary\": summary,\n",
    "            \"image\": image_url or FALLBACK_IMG,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Skipped one article that couldn‚Äôt be processed.\", f\"enrich EXC {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_all(params: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    total_tasks = len(params[\"chosen_sources\"]) + (1 if (params[\"use_newsdata\"] and params[\"newsdata_key\"]) else 0)\n",
    "    total_tasks = max(total_tasks, 1)\n",
    "    progress = st.progress(0.0)\n",
    "    info = st.empty()\n",
    "\n",
    "    # 1) RSS/Atom\n",
    "    for i, src in enumerate(params[\"chosen_sources\"], start=1):\n",
    "        info.info(f\"Fetching RSS {i}/{total_tasks}: {urlparse(src).netloc}\")\n",
    "        try:\n",
    "            raw_items = fetch_from_feed(src, params[\"start_date\"], params[\"end_date\"], params[\"force_fetch\"], params[\"ignore_recency\"])\n",
    "        except Exception as e:\n",
    "            soft_fail(\"Skipped a source due to a transient issue.\", f\"fetch_from_feed EXC {src}: {e}\")\n",
    "            raw_items = []\n",
    "        if params[\"per_source_cap\"] and raw_items:\n",
    "            raw_items = raw_items[:params[\"per_source_cap\"]]\n",
    "\n",
    "        if raw_items:\n",
    "            with ThreadPoolExecutor(max_workers=6) as ex:\n",
    "                futures = [ex.submit(enrich, {**e, \"source\": urlparse(src).netloc}, params[\"keywords\"], params[\"min_relevance\"], params[\"n_sent\"]) for e in raw_items]\n",
    "                for fut in as_completed(futures):\n",
    "                    try:\n",
    "                        r = fut.result()\n",
    "                        if r: rows.append(r)\n",
    "                    except Exception as e:\n",
    "                        soft_fail(\"One article was skipped during processing.\", f\"future enrich EXC {e}\")\n",
    "        progress.progress(min(1.0, i / total_tasks))\n",
    "\n",
    "    # 2) Newsdata.io\n",
    "    if params[\"use_newsdata\"] and params[\"newsdata_key\"]:\n",
    "        info.info(f\"Fetching Newsdata.io {len(params['chosen_sources'])+1}/{total_tasks}\")\n",
    "        try:\n",
    "            nd_items = fetch_from_newsdata(\n",
    "                api_key=params[\"newsdata_key\"],\n",
    "                query=params[\"newsdata_query\"],\n",
    "                start_date=params[\"start_date\"],\n",
    "                end_date=params[\"end_date\"],\n",
    "                language=params[\"nd_language\"] or None,\n",
    "                country=params[\"nd_country\"] or None,\n",
    "                category=params[\"nd_category\"] or None,\n",
    "                max_pages=int(params[\"nd_pages\"]),\n",
    "            )\n",
    "            if params[\"per_source_cap\"] and nd_items:\n",
    "                nd_items = nd_items[:params[\"per_source_cap\"]]\n",
    "            if nd_items:\n",
    "                with ThreadPoolExecutor(max_workers=6) as ex:\n",
    "                    futures = [ex.submit(enrich, it, params[\"keywords\"], params[\"min_relevance\"], params[\"n_sent\"]) for it in nd_items]\n",
    "                    for fut in as_completed(futures):\n",
    "                        try:\n",
    "                            r = fut.result()\n",
    "                            if r: rows.append(r)\n",
    "                        except Exception as e:\n",
    "                            soft_fail(\"One API article was skipped during processing.\", f\"future API enrich EXC {e}\")\n",
    "        except Exception as e:\n",
    "            soft_fail(\"The API was briefly unavailable; results shown are from RSS.\", f\"fetch_from_newsdata EXC {e}\")\n",
    "        progress.progress(1.0)\n",
    "\n",
    "    info.empty()\n",
    "    progress.empty()\n",
    "\n",
    "    # Optional cross-source de-duplication\n",
    "    if params.get(\"dedupe\", True) and rows:\n",
    "        seen = set()\n",
    "        deduped = []\n",
    "        for r in rows:\n",
    "            k = hash_key(r[\"title\"], r[\"link\"])\n",
    "            if k in seen:\n",
    "                continue\n",
    "            seen.add(k)\n",
    "            deduped.append(r)\n",
    "        rows = deduped\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ========================= Card Renderer with functional widgets =========================\n",
    "def render_card(row: pd.Series):\n",
    "    key = f\"card_{hash_key(row['title'], row['link'])}\"\n",
    "    if \"ai_analyses\" not in st.session_state:\n",
    "        st.session_state.ai_analyses = {}\n",
    "\n",
    "    pub = row[\"published\"]\n",
    "    src = row[\"source\"]\n",
    "    rel = f\"{row['relevance']:.0%}\"\n",
    "    title = row[\"title\"]\n",
    "    link = row[\"link\"]\n",
    "    img = row[\"image\"] or FALLBACK_IMG\n",
    "    summary = row[\"auto_summary\"] or \"\"\n",
    "    tags = row[\"impact\"] or [\"General\"]\n",
    "\n",
    "    with st.container():\n",
    "        st.markdown(f\"\"\"\n",
    "        <div class=\"card\">\n",
    "          <img class=\"thumb\" src=\"{img}\" alt=\"thumbnail\">\n",
    "          <div class=\"card-body\">\n",
    "            <div class=\"meta\">{src} ¬∑ {pub} ¬∑ Relevance {rel}</div>\n",
    "            <div class=\"title\">{title}</div>\n",
    "            <div class=\"badges\">\n",
    "                {\"\".join([f'<span class=\"badge\">{t}</span>' for t in tags])}\n",
    "            </div>\n",
    "            <div class=\"summary\">{summary}</div>\n",
    "            <div style=\"margin-top:10px;\">\n",
    "              <a class=\"link\" href=\"{link}\" target=\"_blank\">Read full article ‚Üí</a>\n",
    "            </div>\n",
    "          </div>\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "        with st.expander(\"üîé Analyze with AI\", expanded=False):\n",
    "            if not have_openai():\n",
    "                st.warning(\"Add an `OPENAI_API_KEY` to your `.env` or Streamlit Secrets to run AI analysis.\")\n",
    "                st.info(\"Tip: open the üß™ Diagnostics panel in the sidebar.\")\n",
    "                st.stop()\n",
    "\n",
    "            prev = st.session_state.ai_analyses.get(key)\n",
    "            if prev:\n",
    "                st.markdown(prev)\n",
    "\n",
    "            if st.button(\"Run LLM Analysis\", key=f\"btn_{key}\"):\n",
    "                with st.spinner(\"Analyzing article with AI...\"):\n",
    "                    full_text, _ = fetch_article_text_and_image(link)\n",
    "                    body = full_text if len(full_text) > len(summary) else summary\n",
    "                    if not body:\n",
    "                        st.error(\"Could not extract article text to analyze.\")\n",
    "                    else:\n",
    "                        md = analyze_with_llm(title, body, tags)\n",
    "                        if not md:\n",
    "                            st.error(\"AI analysis failed. Check Diagnostics and try again.\")\n",
    "                        else:\n",
    "                            st.session_state[\"ai_analyses\"][key] = md\n",
    "                            st.markdown(md)\n",
    "\n",
    "def ui_results(df: pd.DataFrame, top_k: int, sent_df: Optional[pd.DataFrame], sent_summary: Dict[str, Any]):\n",
    "    st.subheader(\"üìä Results\")\n",
    "    if df.empty:\n",
    "        st.warning(\"No relevant articles found. Try widening the date range or lowering the relevance threshold.\")\n",
    "    else:\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1:\n",
    "            all_impacts = sorted({t for tags in df[\"impact\"] for t in tags})\n",
    "            impact_filter = st.multiselect(\n",
    "                \"Filter by impact\",\n",
    "                options=all_impacts,\n",
    "                default=ss_get(\"filters_impact\", []),\n",
    "                key=\"filters_impact\"\n",
    "            )\n",
    "        with c2:\n",
    "            source_filter = st.multiselect(\n",
    "                \"Filter by source\",\n",
    "                options=sorted(df[\"source\"].unique()),\n",
    "                default=ss_get(\"filters_source\", []),\n",
    "                key=\"filters_source\"\n",
    "            )\n",
    "\n",
    "        filtered = df.copy()\n",
    "        if impact_filter:\n",
    "            filtered = filtered[filtered[\"impact\"].apply(lambda x: any(t in x for t in impact_filter))]\n",
    "        if source_filter:\n",
    "            filtered = filtered[filtered[\"source\"].isin(source_filter)]\n",
    "\n",
    "        cards = list(filtered.to_dict(\"records\"))\n",
    "        n = 3  # 3 columns\n",
    "        for i in range(0, len(cards), n):\n",
    "            cols = st.columns(n)\n",
    "            for j, col in enumerate(cols):\n",
    "                if i + j < len(cards):\n",
    "                    with col:\n",
    "                        render_card(pd.Series(cards[i + j]))\n",
    "            st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "\n",
    "        st.subheader(\"üìù Daily Digest\")\n",
    "        digest_md = make_digest(filtered if (impact_filter or source_filter) else df, top_k=top_k)\n",
    "        st.markdown(digest_md)\n",
    "\n",
    "        st.subheader(\"‚¨áÔ∏è Downloads\")\n",
    "        ts = dt.datetime.now(dt.timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "        export_df = filtered if (impact_filter or source_filter) else df\n",
    "        csv_name = f\"oneafrica_pulse_{ts}.csv\"\n",
    "        md_name = f\"oneafrica_pulse_digest_{ts}.md\"\n",
    "        st.download_button(\"üì• Download CSV\", data=export_df.to_csv(index=False).encode(\"utf-8\"),\n",
    "                           file_name=csv_name, mime=\"text/csv\")\n",
    "        st.download_button(\"üì• Download Digest (Markdown)\", data=digest_md.encode(\"utf-8\"),\n",
    "                           file_name=md_name, mime=\"text/markdown\")\n",
    "\n",
    "        # NEW: Word (.docx) export button\n",
    "        try:\n",
    "            docx_bytes = build_results_docx(\n",
    "                app_name=APP_NAME,\n",
    "                tagline=TAGLINE,\n",
    "                quote=QUOTE,\n",
    "                results_df=export_df,\n",
    "                digest_md=digest_md,\n",
    "                params=st.session_state.get(\"last_scan_params\", current_params),\n",
    "                sent_summary=sent_summary\n",
    "            )\n",
    "            docx_name = f\"oneafrica_pulse_{ts}.docx\"\n",
    "            st.download_button(\n",
    "                \"üì• Download Report (Word .docx)\",\n",
    "                data=docx_bytes,\n",
    "                file_name=docx_name,\n",
    "                mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            st.error(f\"Could not generate Word report: {e}\")\n",
    "\n",
    "        st.info(\"üí° Tip: Paste the Markdown into an email, WhatsApp (as a code block), or your wiki for quick sharing.\")\n",
    "\n",
    "    # -------- Social Sentiment Section --------\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"üê¶ Social Sentiment ‚Äî Twitter/X\")\n",
    "    if (sent_df is None) or (sent_df is not None and sent_df.empty):\n",
    "        st.caption(\"No tweets captured for the current window/query. Enable in the sidebar and click **Scan Now**.\")\n",
    "    else:\n",
    "        met1, met2, met3, met4 = st.columns(4)\n",
    "        with met1:\n",
    "            st.metric(\"Tweets analyzed\", int(sent_summary.get(\"n\", 0)))\n",
    "        with met2:\n",
    "            st.metric(\"Mean sentiment (compound)\", f\"{sent_summary.get('mean_compound', 0.0):+.3f}\")\n",
    "        with met3:\n",
    "            st.metric(\"Positive\", f\"{100*sent_summary.get('share_pos',0.0):.1f}%\")\n",
    "        with met4:\n",
    "            st.metric(\"Negative\", f\"{100*sent_summary.get('share_neg',0.0):.1f}%\")\n",
    "\n",
    "        share_df = pd.DataFrame({\n",
    "            \"label\": [\"Positive\",\"Neutral\",\"Negative\"],\n",
    "            \"share\": [\n",
    "                100*sent_summary.get(\"share_pos\",0.0),\n",
    "                100*sent_summary.get(\"share_neu\",0.0),\n",
    "                100*sent_summary.get(\"share_neg\",0.0),\n",
    "            ]\n",
    "        })\n",
    "        st.bar_chart(data=share_df, x=\"label\", y=\"share\", use_container_width=True)\n",
    "\n",
    "        with st.expander(\"See recent tweets\"):\n",
    "            show_cols = [\"created_at\",\"label\",\"compound\",\"text\",\"likes\",\"retweets\",\"username\",\"url\"]\n",
    "            st.dataframe(sent_df[show_cols], use_container_width=True, height=320)\n",
    "\n",
    "def friendly_error_summary():\n",
    "    if not SOFT_ERRORS:\n",
    "        return\n",
    "    counts: Dict[str,int] = {}\n",
    "    for m in SOFT_ERRORS:\n",
    "        counts[m] = counts.get(m, 0) + 1\n",
    "    bullets = \"\".join([f\"- {msg} _(x{n})_\\n\" for msg, n in counts.items()])\n",
    "    st.info(f\"\"\"\n",
    "**Heads up:** Some sources were temporarily skipped or partially loaded.  \n",
    "This doesn‚Äôt affect your ability to scan and summarize current items.\n",
    "\n",
    "{bullets}\n",
    "    \"\"\")\n",
    "\n",
    "# ========================= Main (stable, button-gated) =========================\n",
    "st.markdown(CARD_CSS, unsafe_allow_html=True)\n",
    "\n",
    "if 'run_btn' not in locals():\n",
    "    run_btn = False  # safety\n",
    "\n",
    "if run_btn:\n",
    "    try:\n",
    "        if not current_params[\"chosen_sources\"] and not (current_params[\"use_newsdata\"] and current_params[\"newsdata_key\"]):\n",
    "            st.error(\"Pick at least one RSS source or enable Newsdata.io (see Configurations).\")\n",
    "        else:\n",
    "            with st.spinner(\"Scanning sources, extracting content, and generating summaries...\"):\n",
    "                rows = fetch_all(current_params)\n",
    "                df = process_rows(rows)\n",
    "\n",
    "                st.session_state[\"results_df\"] = df\n",
    "                st.session_state[\"results_digest_md\"] = make_digest(df, top_k=current_params[\"top_k\"])\n",
    "                st.session_state[\"last_scan_params\"] = current_params\n",
    "\n",
    "            # ---------- Social Sentiment pass (optional) ----------\n",
    "            if current_params[\"enable_social\"]:\n",
    "                with st.spinner(\"Collecting and analyzing Twitter/X sentiment via Grok...\"):\n",
    "                    q = current_params[\"tw_query\"]\n",
    "                    lang = current_params[\"tw_lang\"]\n",
    "                    hours = current_params[\"tw_hours\"]\n",
    "                    max_t = current_params[\"tw_max\"]\n",
    "\n",
    "                    tweets = fetch_tweets_via_grok(q, lang, hours, max_t)\n",
    "                    if current_params[\"sentiment_method\"] == \"Grok (xAI API)\":\n",
    "                        df_t = analyze_tweet_sentiment_grok(tweets)\n",
    "                    else:\n",
    "                        df_t = analyze_tweet_sentiment(tweets)  # fallback to VADER\n",
    "\n",
    "                    summ = summarize_sentiment(df_t)\n",
    "                    st.session_state[\"sent_df\"] = df_t\n",
    "                    st.session_state[\"sent_summary\"] = summ\n",
    "            else:\n",
    "                st.session_state[\"sent_df\"] = pd.DataFrame()\n",
    "                st.session_state[\"sent_summary\"] = {\"n\": 0, \"mean_compound\": 0.0, \"share_pos\": 0.0, \"share_neu\": 0.0, \"share_neg\": 0.0}\n",
    "\n",
    "    except Exception as e:\n",
    "        soft_fail(\"Something went wrong while assembling the results.\", f\"MAIN EXC {e}\")\n",
    "        st.error(\"We ran into a hiccup assembling the results. Please try again or adjust your filters.\")\n",
    "    finally:\n",
    "        friendly_error_summary()\n",
    "\n",
    "# Render either the newly saved results or the last good results\n",
    "df = st.session_state.get(\"results_df\", None)\n",
    "digest_md = st.session_state.get(\"results_digest_md\", \"\")\n",
    "sent_df = st.session_state.get(\"sent_df\", None)\n",
    "sent_summary = st.session_state.get(\"sent_summary\", {})\n",
    "\n",
    "if df is None:\n",
    "    st.info(\"\"\"\n",
    "**What this demo does:**\n",
    "- üì∞ Scans curated RSS/Atom feeds (+ optional Newsdata.io API) for the last *N* days  \n",
    "- üìë Fetches full article text where possible + **thumbnails** (Open Graph)  \n",
    "- üéØ Scores relevance against **your commodity & policy keywords**  \n",
    "- üìù Auto-summarizes into 2‚Äì6 sentences  \n",
    "- üè∑Ô∏è Tags each item (Supply Risk, FX & Policy, Logistics, etc.)  \n",
    "- üíæ Outputs a **downloadable CSV** and **Daily Digest (Markdown)**\n",
    "- üê¶ (Optional) Collects and analyzes **Twitter/X sentiment** for your query/time window\n",
    "    \"\"\")\n",
    "else:\n",
    "    ui_results(df, top_k=st.session_state.get(\"last_scan_params\", {}).get(\"top_k\", 12),\n",
    "               sent_df=sent_df, sent_summary=sent_summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
